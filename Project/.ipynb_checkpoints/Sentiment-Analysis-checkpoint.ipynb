{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b402b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Libraries import get_twitter_data\n",
    "from Libraries import get_yahoo_data\n",
    "from Libraries import Multiclass_SVM\n",
    "from Libraries import NaiveBayes\n",
    "#\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "#\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "#\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e171ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Day included\n",
    "twitterData = get_twitter_data.TwitterData('2022-04-03', daysBack=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b57e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'AAPL'\n",
    "tweets = twitterData.getTwitterData(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28cb9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not included\n",
    "yahooData = get_yahoo_data.YahooData('2022-04-03', daysBack=60)\n",
    "historical_data = yahooData.getYahooData(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e7c3973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-26</th>\n",
       "      <td>163.500000</td>\n",
       "      <td>164.389999</td>\n",
       "      <td>157.820007</td>\n",
       "      <td>159.690002</td>\n",
       "      <td>159.486801</td>\n",
       "      <td>108275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-27</th>\n",
       "      <td>162.449997</td>\n",
       "      <td>163.839996</td>\n",
       "      <td>158.279999</td>\n",
       "      <td>159.220001</td>\n",
       "      <td>159.017410</td>\n",
       "      <td>121954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-28</th>\n",
       "      <td>165.710007</td>\n",
       "      <td>170.350006</td>\n",
       "      <td>162.800003</td>\n",
       "      <td>170.330002</td>\n",
       "      <td>170.113266</td>\n",
       "      <td>179935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-31</th>\n",
       "      <td>170.160004</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>169.509995</td>\n",
       "      <td>174.779999</td>\n",
       "      <td>174.557602</td>\n",
       "      <td>115541600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>174.009995</td>\n",
       "      <td>174.839996</td>\n",
       "      <td>172.309998</td>\n",
       "      <td>174.610001</td>\n",
       "      <td>174.387817</td>\n",
       "      <td>86213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-02</th>\n",
       "      <td>174.750000</td>\n",
       "      <td>175.880005</td>\n",
       "      <td>173.330002</td>\n",
       "      <td>175.839996</td>\n",
       "      <td>175.616257</td>\n",
       "      <td>84914300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-03</th>\n",
       "      <td>174.479996</td>\n",
       "      <td>176.240005</td>\n",
       "      <td>172.119995</td>\n",
       "      <td>172.899994</td>\n",
       "      <td>172.679993</td>\n",
       "      <td>89418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-04</th>\n",
       "      <td>171.679993</td>\n",
       "      <td>174.100006</td>\n",
       "      <td>170.679993</td>\n",
       "      <td>172.389999</td>\n",
       "      <td>172.389999</td>\n",
       "      <td>82465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-07</th>\n",
       "      <td>172.860001</td>\n",
       "      <td>173.949997</td>\n",
       "      <td>170.949997</td>\n",
       "      <td>171.660004</td>\n",
       "      <td>171.660004</td>\n",
       "      <td>77251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-08</th>\n",
       "      <td>171.729996</td>\n",
       "      <td>175.350006</td>\n",
       "      <td>171.429993</td>\n",
       "      <td>174.830002</td>\n",
       "      <td>174.830002</td>\n",
       "      <td>74829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-09</th>\n",
       "      <td>176.050003</td>\n",
       "      <td>176.649994</td>\n",
       "      <td>174.899994</td>\n",
       "      <td>176.279999</td>\n",
       "      <td>176.279999</td>\n",
       "      <td>71285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-10</th>\n",
       "      <td>174.139999</td>\n",
       "      <td>175.479996</td>\n",
       "      <td>171.550003</td>\n",
       "      <td>172.119995</td>\n",
       "      <td>172.119995</td>\n",
       "      <td>90865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-11</th>\n",
       "      <td>172.330002</td>\n",
       "      <td>173.080002</td>\n",
       "      <td>168.039993</td>\n",
       "      <td>168.639999</td>\n",
       "      <td>168.639999</td>\n",
       "      <td>98670700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-14</th>\n",
       "      <td>167.369995</td>\n",
       "      <td>169.580002</td>\n",
       "      <td>166.559998</td>\n",
       "      <td>168.880005</td>\n",
       "      <td>168.880005</td>\n",
       "      <td>86185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-15</th>\n",
       "      <td>170.970001</td>\n",
       "      <td>172.949997</td>\n",
       "      <td>170.250000</td>\n",
       "      <td>172.789993</td>\n",
       "      <td>172.789993</td>\n",
       "      <td>62527400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-16</th>\n",
       "      <td>171.850006</td>\n",
       "      <td>173.339996</td>\n",
       "      <td>170.050003</td>\n",
       "      <td>172.550003</td>\n",
       "      <td>172.550003</td>\n",
       "      <td>61177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-17</th>\n",
       "      <td>171.029999</td>\n",
       "      <td>171.910004</td>\n",
       "      <td>168.470001</td>\n",
       "      <td>168.880005</td>\n",
       "      <td>168.880005</td>\n",
       "      <td>69589300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-18</th>\n",
       "      <td>169.820007</td>\n",
       "      <td>170.539993</td>\n",
       "      <td>166.190002</td>\n",
       "      <td>167.300003</td>\n",
       "      <td>167.300003</td>\n",
       "      <td>82772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-22</th>\n",
       "      <td>164.979996</td>\n",
       "      <td>166.690002</td>\n",
       "      <td>162.149994</td>\n",
       "      <td>164.320007</td>\n",
       "      <td>164.320007</td>\n",
       "      <td>91162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-23</th>\n",
       "      <td>165.539993</td>\n",
       "      <td>166.149994</td>\n",
       "      <td>159.750000</td>\n",
       "      <td>160.070007</td>\n",
       "      <td>160.070007</td>\n",
       "      <td>90009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-24</th>\n",
       "      <td>152.580002</td>\n",
       "      <td>162.850006</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>162.740005</td>\n",
       "      <td>162.740005</td>\n",
       "      <td>141147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2022-01-26  163.500000  164.389999  157.820007  159.690002  159.486801   \n",
       "2022-01-27  162.449997  163.839996  158.279999  159.220001  159.017410   \n",
       "2022-01-28  165.710007  170.350006  162.800003  170.330002  170.113266   \n",
       "2022-01-31  170.160004  175.000000  169.509995  174.779999  174.557602   \n",
       "2022-02-01  174.009995  174.839996  172.309998  174.610001  174.387817   \n",
       "2022-02-02  174.750000  175.880005  173.330002  175.839996  175.616257   \n",
       "2022-02-03  174.479996  176.240005  172.119995  172.899994  172.679993   \n",
       "2022-02-04  171.679993  174.100006  170.679993  172.389999  172.389999   \n",
       "2022-02-07  172.860001  173.949997  170.949997  171.660004  171.660004   \n",
       "2022-02-08  171.729996  175.350006  171.429993  174.830002  174.830002   \n",
       "2022-02-09  176.050003  176.649994  174.899994  176.279999  176.279999   \n",
       "2022-02-10  174.139999  175.479996  171.550003  172.119995  172.119995   \n",
       "2022-02-11  172.330002  173.080002  168.039993  168.639999  168.639999   \n",
       "2022-02-14  167.369995  169.580002  166.559998  168.880005  168.880005   \n",
       "2022-02-15  170.970001  172.949997  170.250000  172.789993  172.789993   \n",
       "2022-02-16  171.850006  173.339996  170.050003  172.550003  172.550003   \n",
       "2022-02-17  171.029999  171.910004  168.470001  168.880005  168.880005   \n",
       "2022-02-18  169.820007  170.539993  166.190002  167.300003  167.300003   \n",
       "2022-02-22  164.979996  166.690002  162.149994  164.320007  164.320007   \n",
       "2022-02-23  165.539993  166.149994  159.750000  160.070007  160.070007   \n",
       "2022-02-24  152.580002  162.850006  152.000000  162.740005  162.740005   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2022-01-26  108275300  \n",
       "2022-01-27  121954600  \n",
       "2022-01-28  179935700  \n",
       "2022-01-31  115541600  \n",
       "2022-02-01   86213900  \n",
       "2022-02-02   84914300  \n",
       "2022-02-03   89418100  \n",
       "2022-02-04   82465400  \n",
       "2022-02-07   77251200  \n",
       "2022-02-08   74829200  \n",
       "2022-02-09   71285000  \n",
       "2022-02-10   90865900  \n",
       "2022-02-11   98670700  \n",
       "2022-02-14   86185500  \n",
       "2022-02-15   62527400  \n",
       "2022-02-16   61177400  \n",
       "2022-02-17   69589300  \n",
       "2022-02-18   82772700  \n",
       "2022-02-22   91162800  \n",
       "2022-02-23   90009200  \n",
       "2022-02-24  141147500  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c6f4260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect tweet and process twitter corpus....\n"
     ]
    }
   ],
   "source": [
    "yahoo_open_price = {}\n",
    "yahoo_close_price = {}\n",
    "yahoo_high_price = {}\n",
    "yahoo_low_price = {}\n",
    "yahoo_volume_price = {}\n",
    "\n",
    "for i in range(len(historical_data)):\n",
    "    #date = historical_data.index[i]\n",
    "    date_o = datetime.datetime.strptime(str(historical_data.index[i]), '%Y-%m-%d %H:%M:%S')\n",
    "    date = date_o.strftime('%Y-%m-%d')\n",
    "    yahoo_open_price.update({date: historical_data.iloc[i]['Open']})\n",
    "    yahoo_close_price.update({date: historical_data.iloc[i]['Close']})\n",
    "    yahoo_high_price.update({date: historical_data.iloc[i]['High']})\n",
    "    yahoo_low_price.update({date: historical_data.iloc[i]['Low']})\n",
    "    yahoo_volume_price.update({date: historical_data.iloc[i]['Volume']})\n",
    "\n",
    "print(\"Collect tweet and process twitter corpus....\")\n",
    "tweet_s = []\n",
    "for key,val in tweets.items():\n",
    "    for value in val:\n",
    "        tweet_s.append(value)\n",
    "\n",
    "csvFile = open('../Data/SampleTweets.csv', 'w',encoding='utf8',newline='')\n",
    "csvWriter = csv.writer(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb65df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceTwoOrMore(s):\n",
    "    # look for 2 or more repetitions of character\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17949007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a453d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStopWordList(stopWordListFileName):\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d98bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureVector(tweet, stopWords):\n",
    "    featureVector = []\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        w = replaceTwoOrMore(w)\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*[a-zA-Z]+[a-zA-Z0-9]*$\", w)\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a257927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "963c7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureVectorAndLabels(tweets, featureList):\n",
    "    sortedFeatures = sorted(featureList)\n",
    "    map = {}\n",
    "    feature_vector = []\n",
    "    labels = []\n",
    "    file = open(\"newfile.txt\", \"w\")\n",
    "\n",
    "    for t in tweets:\n",
    "        label = 0\n",
    "        map = {}\n",
    "        for w in sortedFeatures:\n",
    "            map[w] = 0\n",
    "\n",
    "        tweet_words = t[0]\n",
    "        tweet_opinion = t[1]\n",
    "\n",
    "        # Fill the map\n",
    "        for word in tweet_words:\n",
    "            word = replaceTwoOrMore(word)\n",
    "            word = word.strip('\\'\"?,.')\n",
    "            if word in map:\n",
    "                map[word] = 1\n",
    "        # end for loop\n",
    "        values = map.values()\n",
    "        feature_vector.append(values)\n",
    "        if (tweet_opinion == '|positive|'):\n",
    "            label = 0\n",
    "            tweet_opinion = 'positive'\n",
    "        elif (tweet_opinion == '|negative|'):\n",
    "            label = 1\n",
    "            tweet_opinion = 'negative'\n",
    "        elif (tweet_opinion == '|neutral|'):\n",
    "            label = 2\n",
    "            tweet_opinion = 'neutral'\n",
    "        labels.append(label)\n",
    "        #print(str(list(values)).strip('[]'))\n",
    "        feature_vector_value = str(list(values)).strip('[]')\n",
    "        line = feature_vector_value + \",\" + str(label) + \"\\n\"\n",
    "        file.write(line)\n",
    "    file.close()\n",
    "    return {'feature_vector' : feature_vector, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50eed5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the AFINN lexicon, unzip, and read the latest word list in AFINN-111.txt\n",
    "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
    "zipfile = ZipFile(BytesIO(url.read()))\n",
    "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
    "\n",
    "afinn = dict()\n",
    "for line in afinn_file:\n",
    "    parts = line.decode(\"utf-8\").strip().split()\n",
    "    if len(parts) == 2:\n",
    "        afinn[parts[0]] = int(parts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4c8636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.sub('\\W+', ' ', text.lower()).split()\n",
    "\n",
    "def afinn_sentiment(terms, afinn):\n",
    "\n",
    "    total = 0.\n",
    "    for t in terms:\n",
    "        if t in afinn:\n",
    "            total += afinn[t]\n",
    "    return total\n",
    "\n",
    "def sentiment_analyzer():\n",
    "    tokens = [tokenize(t) for t in tweet_s]  # Tokenize all the tweets\n",
    "\n",
    "    afinn_total = []\n",
    "    for tweet in tokens:\n",
    "        total = afinn_sentiment(tweet, afinn)\n",
    "        afinn_total.append(total)\n",
    "\n",
    "    positive_tweet_counter = []\n",
    "    negative_tweet_counter = []\n",
    "    neutral_tweet_counter = []\n",
    "    for i in range(len(afinn_total)):\n",
    "        decode_tweet = str(tweet_s[i]).split(\"|\")#str(tweet_s[i].encode('utf-8')).split(\"|\")\n",
    "        #print(type(tweet_s[i]))\n",
    "        if afinn_total[i] > 0:\n",
    "            positive_tweet_counter.append(afinn_total[i])\n",
    "            csvWriter.writerow([\"|positive|\", decode_tweet[0], decode_tweet[1], float(afinn_total[i])])\n",
    "        elif afinn_total[i] < 0:\n",
    "            negative_tweet_counter.append(afinn_total[i])\n",
    "            csvWriter.writerow([\"|negative|\", decode_tweet[0], decode_tweet[1], float(afinn_total[i])])\n",
    "        else:\n",
    "            neutral_tweet_counter.append(afinn_total[i])\n",
    "            csvWriter.writerow([\"|neutral|\", decode_tweet[0], decode_tweet[1], float(afinn_total[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bae47ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tweets and store in CSV file ....\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing tweets and store in CSV file ....\")\n",
    "sentiment_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d204c26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset....\n",
      "Creating feature set and generating feature matrix....\n"
     ]
    }
   ],
   "source": [
    "print (\"Preparing dataset....\")\n",
    "# Read the tweets one by one and process it\n",
    "inpTweets = csv.reader(open('../Data/SampleTweets.csv', 'r',encoding='utf-8'), delimiter=',')\n",
    "stopWords = getStopWordList('../Data/stopwords.txt')\n",
    "count = 0;\n",
    "featureList = []\n",
    "labelList = []\n",
    "tweets = []\n",
    "dates =[]\n",
    "date_split =[]\n",
    "list_tweet = []\n",
    "print (\"Creating feature set and generating feature matrix....\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47876e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in inpTweets:\n",
    "    if len(row) == 4:\n",
    "        list_tweet.append(row)\n",
    "        sentiment = row[0]\n",
    "        date = row[1]\n",
    "        t = row[2]\n",
    "\n",
    "        date_split.append(date)\n",
    "        dates.append(date)\n",
    "        labelList.append(sentiment)\n",
    "        processedTweet = processTweet(t)\n",
    "        featureVector = getFeatureVector(processedTweet, stopWords)\n",
    "        featureList.extend(featureVector)\n",
    "        tweets.append((featureVector, sentiment));\n",
    "    else:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f603c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getFeatureVectorAndLabels(tweets, featureList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f009a",
   "metadata": {},
   "source": [
    "print(\"Dataset is ready \\n\")\n",
    "print(\"Sentiment prediction using Naive Bayes Bernoulli and SVM model....\")\n",
    "# Naive Bernoulli and SVM Algorithm\n",
    "data2 = open('newfile.txt', 'r')\n",
    "\n",
    "inp_data2 = []\n",
    "files = np.loadtxt(data2,dtype=str, delimiter=',')\n",
    "\n",
    "inp_data2 = np.array(files[:,0:-1], dtype='float')\n",
    "givenY = files[:,-1]\n",
    "\n",
    "target2=np.zeros(len(givenY), dtype='int')\n",
    "unique_y = np.unique(givenY)\n",
    "\n",
    "for cls in range(len(givenY)):\n",
    "    for x in range(len(unique_y)):\n",
    "        if(givenY[cls] == unique_y[x]):\n",
    "            target2[cls] = x\n",
    "\n",
    "X = np.array(inp_data2)\n",
    "y = np.array(target2)\n",
    "\n",
    "# print type(X)\n",
    "# print type(y)\n",
    "\n",
    "max_gX = {}\n",
    "maximum_gX = []\n",
    "temp = 0\n",
    "svn_temp = 0\n",
    "final_precision=0\n",
    "final_recall = 0\n",
    "final_fmeasure = 0\n",
    "final_accuracy = 0\n",
    "\n",
    "svm_final_accuracy = 0\n",
    "svm_final_precision = 0\n",
    "svm_final_recall = 0\n",
    "svm_final_fmeasure = 0\n",
    "\n",
    "svm_accuracy = []\n",
    "NB_accuracy = []\n",
    "NBSKL_accuracy = []\n",
    "kf = KFold(6, shuffle=False)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # SVM Start\n",
    "    \n",
    "    clf = Multiclass_SVM.MulticlassSVM(C=0.1, tol=0.01, max_iter=100, random_state=0, verbose=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted_y = clf.calculate_prediction(X_test)\n",
    "    svm_accuracy.append(accuracy_score(y_test,predicted_y))\n",
    "    svm_confusion_mat = confusion_matrix(y_test, predicted_y)\n",
    "    sv_accuracy, svm_precision_val, svm_recall_val, svm_f_measure_val = clf.svm_findOtherParameters(svm_confusion_mat)\n",
    "\n",
    "    # SVM end\n",
    "    \n",
    "    # Naive Bayes\n",
    "\n",
    "    clf_NB = BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "    clf_NB.fit(X_train, y_train)\n",
    "    predictedBNB_y = clf_NB.predict(X_test)\n",
    "    NBSKL_accuracy.append(accuracy_score(y_test,predictedBNB_y))\n",
    "    \n",
    "    nb = NaiveBayes.NaiveBayesBernoulli()\n",
    "    \n",
    "    # iterate data for each class\n",
    "    for clas in np.unique(y):\n",
    "        class_feature_matrix = X_train[y_train==clas]\n",
    "        prior_array = len(class_feature_matrix)*1.0/len(X_train)\n",
    "        # print prior_array\n",
    "        alpha = [(np.sum(class_feature_matrix[:,i])/len(class_feature_matrix)) for i in range(class_feature_matrix.shape[1])]\n",
    "        gX = nb.membership_function(X_test, alpha, prior_array)\n",
    "        max_gX.update({int(clas): gX})\n",
    "    # find discriminant function\n",
    "    disc_function = nb.discriminant_function(max_gX, np.unique(y))\n",
    "    # print disc_function\n",
    "    confusion_mat = confusion_matrix(y_test, predictedBNB_y)\n",
    "    # print confusion_mat\n",
    "\n",
    "    # find precision, recall , f-measure\n",
    "    accuracy, precision_val, recall_val, f_measure_val = nb.findOtherParameters(confusion_mat)\n",
    "\n",
    "    if accuracy_score(y_test,predictedBNB_y) > temp:\n",
    "        if (accuracy_score(y_test,predictedBNB_y) != 1):\n",
    "            final_accuracy = accuracy_score(y_test,predictedBNB_y)\n",
    "            final_precision = precision_val\n",
    "            final_recall = recall_val\n",
    "            final_fmeasure = f_measure_val\n",
    "            temp = accuracy_score(y_test,predictedBNB_y)\n",
    "\n",
    "    if accuracy_score(y_test,predicted_y) > svn_temp:\n",
    "        if (accuracy_score(y_test,predicted_y) != 1):\n",
    "            svm_final_accuracy = accuracy_score(y_test,predicted_y)\n",
    "            svm_final_precision = svm_precision_val\n",
    "            svm_final_recall = svm_recall_val\n",
    "            svm_final_fmeasure = svm_f_measure_val\n",
    "            svn_temp = accuracy_score(y_test,predicted_y)\n",
    "\n",
    "    # Naive Bayes end\n",
    "    \n",
    "print(\"Finished...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eefbbb",
   "metadata": {},
   "source": [
    "print (\"Bernoulli NB\")\n",
    "print (\"Accuracy =\" ,max(NBSKL_accuracy))\n",
    "print (\"Precision = \", final_precision)\n",
    "print (\"Recall = \", final_recall)\n",
    "print (\"F-Measure\", final_fmeasure)\n",
    "print (\"\\n\")\n",
    "print (\"SVM\")\n",
    "print (\"Accuracy =\", max(svm_accuracy))\n",
    "print (\"Precision = \", svm_final_precision)\n",
    "print (\"Recall = \", svm_final_recall)\n",
    "print (\"F-Measure\", svm_final_fmeasure)\n",
    "print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69c404d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for stock prediction using yahoo finance and tweet sentiment....\n",
      "Dataset is ready for stock prediction \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Preparing dataset for stock prediction using yahoo finance and tweet sentiment....\")\n",
    "date_tweet_details = {}\n",
    "file = open(\"stockpredict.txt\", \"w\")\n",
    "for dateVal in np.unique(date_split):\n",
    "    ini_date = dateVal.strip()\n",
    "    date_totalCount = 0\n",
    "    date_PosCount = 0\n",
    "    date_NegCount = 0\n",
    "    date_NutCount = 0\n",
    "    total_sentiment_score = 0.\n",
    "    for row in list_tweet:\n",
    "        sentiment = row[0]\n",
    "        temp_date = row[1]\n",
    "        sentiment_score = row[3]\n",
    "        if(temp_date == dateVal):\n",
    "            total_sentiment_score += float(sentiment_score)\n",
    "            date_totalCount+=1\n",
    "            if (sentiment == '|positive|'):\n",
    "                date_PosCount+=1\n",
    "            elif (sentiment == '|negative|'):\n",
    "                date_NegCount+=1\n",
    "            elif (sentiment == '|neutral|'):\n",
    "                date_NutCount+=1\n",
    "\n",
    "    s = str(date_totalCount)+\" \"+str(date_PosCount)+\" \"+str(date_NegCount)+\" \"+str(date_NutCount)\n",
    "    date_tweet_details.update({dateVal: s})\n",
    "    dateVal = dateVal.strip()\n",
    "    day = datetime.datetime.strptime(dateVal, '%Y-%m-%d').strftime('%A')\n",
    "    closing_price = 0.\n",
    "    opening_price = 0.\n",
    "    high_price = 0.\n",
    "    low_price = 0.\n",
    "    volume = 0\n",
    "    market_status = 0\n",
    "    if day == 'Saturday':\n",
    "        update_date = dateVal.split(\"-\")\n",
    "        if len(str((int(update_date[2])-1)))==1:\n",
    "            dateVal = update_date[0]+\"-\"+update_date[1]+\"-0\"+str((int(update_date[2])-1))\n",
    "        else:\n",
    "            dateVal = update_date[0] + \"-\" + update_date[1] + \"-\" + str((int(update_date[2]) - 1))\n",
    "    elif day == 'Sunday':\n",
    "        update_date = dateVal.split(\"-\")\n",
    "        if len(str((int(update_date[2])-2)))==1:\n",
    "            dateVal = update_date[0]+\"-\"+update_date[1]+\"-0\"+str((int(update_date[2])-2))\n",
    "        else:\n",
    "            dateVal = update_date[0] + \"-\" + update_date[1] + \"-\" + str((int(update_date[2]) - 2))\n",
    "    \n",
    "\n",
    "    if dateVal in yahoo_open_price:\n",
    "        yahoo_close_price[ini_date] = yahoo_close_price[dateVal]\n",
    "        yahoo_open_price[ini_date] = yahoo_open_price[dateVal]\n",
    "        yahoo_high_price[ini_date] = yahoo_high_price[dateVal]\n",
    "        yahoo_low_price[ini_date] = yahoo_low_price[dateVal]\n",
    "        yahoo_volume_price[ini_date] = yahoo_volume_price[dateVal]\n",
    "        \n",
    "        if (float(closing_price)-float(opening_price)) > 0:\n",
    "            market_status = 1\n",
    "        else:\n",
    "            market_status =-1\n",
    "        file.write( str(date_PosCount) + \",\" + str(date_NegCount) + \",\" + str(date_NutCount) +\",\" + str(date_totalCount) + \",\" + str(market_status) + \"\\n\")\n",
    "\n",
    "file.close()\n",
    "print (\"Dataset is ready for stock prediction \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99daa29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Libraries import StockPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34345425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating feature matrix\n",
    "#percentage_positive_sentiment\n",
    "\n",
    "sentiment_dic = {}\n",
    "\n",
    "for key in date_tweet_details:\n",
    "    date = key.strip()\n",
    "    if date in yahoo_close_price:\n",
    "        sentiment_score = date_tweet_details[key].split(' ')\n",
    "        total_s = float(sentiment_score[0])\n",
    "        positive_s = float(sentiment_score[1])/total_s\n",
    "        negative_s = float(sentiment_score[2])/total_s\n",
    "        neutral_s = float(sentiment_score[3])/total_s\n",
    "            \n",
    "        HLPCT = (yahoo_high_price[date] - yahoo_low_price[date]) / yahoo_low_price[date]\n",
    "        PCT = (yahoo_close_price[date] - yahoo_open_price[date]) / yahoo_open_price[date]\n",
    "        \n",
    "        #sentiment_dic[key] = [positive_s, negative_s, neutral_s, HLPCT, PCT, yahoo_volume_price[date], yahoo_close_price[date]]\n",
    "        sentiment_dic[key] = [positive_s, negative_s, neutral_s, HLPCT, PCT, yahoo_volume_price[date], yahoo_close_price[date]]\n",
    "\n",
    "#print(sentiment_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7786eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data for testing and training\n",
    "X_analysys = []\n",
    "Y_analysys = []\n",
    "\n",
    "for key in sentiment_dic:\n",
    "    X_analysys.append(np.array(sentiment_dic[key][:-1]))\n",
    "    Y_analysys.append(sentiment_dic[key][-1:][0])\n",
    "\n",
    "#print(X_analysys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae6944ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(X_analysys, Y_analysys, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d7c9d",
   "metadata": {},
   "source": [
    "svm = Multiclass_SVM.MulticlassSVM(C=0.1, tol=0.01, max_iter=100, random_state=0, verbose=1)\n",
    "svm.fit(np.array(X_train_a), np.array(y_train_a))\n",
    "predicted_y = svm.calculate_prediction(X_test_a)\n",
    "#svm_accuracy.append(accuracy_score(y_test,predicted_y))\n",
    "#svm_confusion_mat = confusion_matrix(y_test, predicted_y)\n",
    "#sv_accuracy, svm_precision_val, svm_recall_val, svm_f_measure_val = clf.svm_findOtherParameters(svm_confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fde815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f29aff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using another library\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#making y train continuous\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_a_labeled = label_encoder.fit_transform(np.array(y_train_a))\n",
    "\n",
    "poly = svm.SVC(kernel='linear', C=0.01, decision_function_shape='ovr').fit(X_train_a, y_train_a_labeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "314c8859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([168.88000488, 168.88000488, 168.88000488, 172.11999512,\n",
       "       172.8999939 , 159.22000122])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preditec_y = poly.predict(X_test_a)\n",
    "\n",
    "label_encoder.inverse_transform(preditec_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ebe0850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[167.3000030517578,\n",
       " 167.3000030517578,\n",
       " 167.3000030517578,\n",
       " 164.32000732421875,\n",
       " 160.07000732421875,\n",
       " 162.74000549316406]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
